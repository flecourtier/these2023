0. Bonjour tout le monde. Pour ceux qui ne me connaissent pas, je m'appelle Frédérique Lecourtier. Je suis doctorante en première année dans l'équipe Mimesis de l'Inria sous la supervision d'Emmanuel Franck, Michel Duprez et Vanessa Lleras. Aujourd'hui, je vais vous parler d'un sujet en lien avec ma thèse qui se base sur les éléments de cours d'Emmanuel et que j'ai nommé Mesh-Based Methods and Physically Informed Learning. 

INTRODUCTION:
T : Commençons par une rapide introduction.

1. On se place dans le contexte scientifique de la création en temps réel de jumeaux numériques d'organes décrits par une fonction LevelSet. Cette fonction LevelSet peut être facilement obtenue à partir d'imagerie médicale. Pour cela, on va considérer la méthode PhiFEM qui est une nouvelle méthode d'éléments finis à domaine fictif développé par une partie de l'équipe Mimesis. Je vais donc vous présenter quelques points importants. Tout d'abord, le domaine sera donné par une fonction Levelset, ce qui signifie qu'on n'aura pas besoin d'un maillage qui va fitter avec le bord de notre domaine. Ensuite, ça va permettre de travailler sur des géométries complexes mais aussi assurer la qualité géométrique du maillage. Les cas pratiques peuvent être par exemple : des simulations temps réel ou encore optimisation de forme.

2. Donc maintenant que je vous ai parlé du contexte scientifique, j'aimerais vous présenter un petit peu l'objectif actuel qui va être le développement de méthodes hybrides qui combinent méthodes éléments finis et réseaux de neurones. La première, la phase offline, où on va considérer plusieurs géométries et plusieurs forces, et entraîner un réseau de neurones appelé PINNs (Physically Informed Neural Networks), à apprendre la solution de notre EDP. Dans la seconde phase qui est notre phase online, on va pouvoir récupérer la prédiction du PINNs sur une seule géométrie et une seule force et y appliquer ce qu'on va appeler une correction. En fait, l'objectif, c'est de corriger la prédiction du réseau avec la méthode PhiFEM pour améliorer la solution, notamment à certains endroits où le réseau aurait pu se tromper. Ici, j'ai noté l'évolution du travail. En fait, pour l'instant, on va considérer des géométries très simples et fixes, comme des cercles en 2D, une EDP qui va être très simple, à savoir le problème de poisson, et un réseau de neurones qui est un PINNs. Mais l'objectif, c'est de complexifier tout ça dans le futur. 

3. Dans cette présentation, on va s'intéresser à un problème elliptique avec condition de Dirichlet, qui va être défini de la manière suivante avec A une condition de coercivité définie positive et c un scalaire. On voit que pour simplifier on a enlever le terme du premier ordre ici.

4. Comme je le disais précédemment, l'objectif de cette présentation n'est pas de vous présenter mon travail de thèse, mais de vous montrer que la philosophie derrière la plupart des méthodes numériques est la même, à savoir plus précisément les méthodes basées sur des maillages et l'apprentissage physiquement informé. L'idée d'une méthode numérique est toute simple : on veut discrétiser un problème de dimension infinie, où notre inconnu est une fonction, en un problème dans un espace de dimension finie, où notre inconnu sera un vecteur. Pour cela, on va considéré trois étapes :
- Tout d'abord l'encodage qui va consister à encoder le problème dans un espace de dimension finie. Cette étape consiste à passer de nos forces f, qui sont des fonctions, à theta_f qui est un vecteur. 
- Ensuite, on va avoir l'approximation qui va consister à résoudre notre problème dans cet espace de dimension finie et passer de theta_f à theta_u qui représente notre vecteur de solution. 
- Et finalement, la dernière étape, qui est l'étape de décodage, va permettre de ramener notre solution dans notre espace de dimension infini, et donc de passer de notre vecteur de solution theta_u à une solution u_theta, qui va être une fonction. 
A noter que ce qu'on appelle un projecteur, ce n'est rien d'autre que la combinaison de l'encodeur et du décodeur. Avec l'encodeur, on passe de f à theta_f puis le decodeur nous fait passer de theta_f à f_theta. La projection n'est autre que le passage de f à f_theta.

MESH-BASED METHODS
T : On va maintenant s'intéresser à ces trois étapes dans le cadre des méthodes basés sur des maillages. 

- ENCODING/DECODING
T : Et dans un premier temps les étapes d'encodage et de décodage.

5. Dans cette section, on va considérer uniquement la méthode élément fini standard. On commence par définir le décodeur (qui est donc l'étape finale) car c'est lui qui nous donne l'espace d'approximation dans lequel on va travailler. Ici, le décodeur qui nous donne u_theta s'écrit comme la combinaison linéaire de nos fonctions de base polynomiales par morceaux phi_i. Comme on considère un décodeur linéaire, notre espace d'approximation, que l'on notera V_N est un espace vectoriel et ainsi on a l'existence et l'unicité d'un projecteur, le projecteur orthogonal. L'encodeur quant à lui consiste en un processus d'optimisation décrit par cette formule * qui nous fournit theta_f. Et comme on se place dans le cadre d'espace vectoriel, ça revient à effectuer la projection orthogonale sur notre espace V_N qui va être décrite par la multiplication matricielle suivante *.

- APPROXIMATION
T : Maintenant que nous avons défini l'encodeur et le décodeur, nous pouvons passer à l'approximateur. 

6. L'idée de l'approximateur consiste à projeter une certaine forme de notre équation sur notre espace vectoriel V_N. Pour cela, on va introduire le résidu de notre équation défini de la manière suivante * avec R_in le résidu à l'intérieur de Omega et R_bc le résidu sur le bord dOmega. Alors, le problème continu écrit à gauche, où J est une fonctionnelle à minimiser, se ramène à un problème sur les degrés de liberté (donc un problème discret). On va alors distinguer deux variantes. La première concernera uniquement les EDP spatiales symétriques où notre fonctionnelle J prendra la forme énergétique de notre EDP, ce qui correspondra à la projection de Gallerkin. La seconde variante est valable pour tout type d'EDP et consiste cette fois-ci à considérer notre fonctionnelle J comme la forme moindre-carré de notre EDP, ce qui correspondra à la projection de Gallerkin moindre-carré. 

7. Commençons par le cas énergétique. On peut écrire notre problème de minimisation sous la forme suivante * avec J_in la forme énergétique de notre EDP définit sur Omega de la manière suivante et J_bc définit sur D-Omega ainsi *. On peut facilement montrer que ce problème de minimisation est équivalent à notre EDP. En fait, en calculant le gradient de notre fonctionnelle J par rapport à v, on retrouve le résidu de notre équation. Et ainsi, si u_theta est solution du problème de minimisation, on va avoir le gradient de J qui est égal à 0, et donc le résidu à l'intérieur de Omega est égal à 0, et le résidu au bord aussi. Ce qui revient en fait à l'équation suivante et ainsi u_theta est solution de notre EDP. 

8. Maintenant que nous avons vérifier que notre problème de minimisation continue est équivalent à résoudre notre EDP, on peut discrétiser celui-ci de la manière suivante *. A noter qu'ici nous ferons abstraction du terme de bord pour faciliter les explications. On définit la projection de Gallerkin de la manière suivante et on peut montrer de la même manière que sur la slide précédente que la projection de Gallerkin est équivalente à résoudre notre EDP. Cette fois-ci, nous calculons le gradient de J par rapport à theta et nous obtenons la formule suivante *. Et ainsi, si u_theta est solution de notre EDP, u_theta est solution du problème de minimisation continue défini sur la slide précédente. Ainsi, theta_u est solution du problème de minimisation discret défini ici. Et finalement, résoudre ce problème de minimisation décrit précisément la projection de Galerkin décrit précédemment. 

9. On va considérer maintenant la forme moindre-carré. On écrit alors notre problème de minimisation sous la forme suivante * avec J_in la forme moindre-carré de notre EDP définit sur Omega de la manière suivante et J_bc définit sur D-Omega comme précédemment. On peut facilement montrer que ce problème de minimisation est équivalent à notre EDP. Cette fois-ci, le gradient de notre fonctionnelle J par rapport à v est égal à le terme de gauche de notre EDP que l'on a noté L, appliqué au résidu de l'équation. Et ainsi, si u_theta est solution du problème de minimisation, on va avoir le gradient de J qui est égal à 0, et donc le résidu à l'intérieur de Omega est égal à 0, et le résidu au bord aussi. Ce qui revient en fait à l'équation suivante et ainsi u_theta est solution de notre EDP. 

10. Maintenant que nous avons vérifier que notre problème de minimisation continue est équivalent à résoudre notre EDP, on peut discrétiser celui-ci de la manière suivante *. On définit la projection de Gallerkin moindre-carré de la manière suivante et on peut montrer que la projection de Gallerkin est équivalente à résoudre notre EDP. Cette fois-ci, nous calculons le gradient de J par rapport à theta et nous obtenons la formule suivante *. Et ainsi, si u_theta est solution de notre EDP, u_theta est solution du problème de minimisation continue défini sur la slide précédente. Ainsi, theta_u est solution du problème de minimisation discret défini ici. Et finalement, résoudre ce problème de minimisation décrit précisément la projection de Galerkin moindre-carré décrit ici.

11. À présent, on peut décomposer la méthode élément fini standard en les trois étapes suivantes. La première étape consistera à encoder la force F par la multiplication matricelle suivante. Puis, on va avoir l'approximateur qui permettra de résoudre le problème en utilisant soit la projection de galerkin définie ici, soit la projection de galèrkin moindre-carré définie ici. Puis finalement, on utilisera le décodeur, définit comme une combinaison linéaire de fonctions polynomiales pour revenir à notre espace en dimension infinie et ainsi obtenir notre solution u_theta. En pratique, l'approximation s'écrit comme un système linéaire avec comme inconnu theta_u et par exemple dans le cadre de la projection de Galerkin, on résout le système linéaire suivant (en omettant ici les conditions de bord).

PHYSICALLY-INFORMED LEARNING
T : On va maintenant s'intéresser à ces trois-mêmes étapes dans le cadre de l'apprentissage physiquement informé.

- ENCODING/DECODING
T : Et dans un premier temps les étapes d'encodage et de décodage.

12. Comme pour FEM standard, on commence par définir le décodeur car c'est lui qui nous donne l'espace d'approximation dans lequel on va travailler. Ici, le décodeur qui nous donne u_theta s'écrit comme un réseau de neurones u_NN (du type MLP par exemple). Comme on considère un décodeur non-linéaire, notre espace d'approximation n'est plus un espace vectoriel mais une variété de dimension finie et ainsi il n'existe pas de projecteur unique. Ensuite, l'encodeur consiste en le processus d'optimisation décrit par cette formule * qui nous fournit theta_f. 

13. On peut parler maintenant de quelques avantages à avoir un décodeur non linéaire, comme les réseau de neurones. Tout d'abord, on va gagner en richesse d'approximation. Ensuite, on peut espérer réduire significativement le nombre de degrés de liberté et finalement ça nous évite l'utilisation d'un maillage. En fait les modèles polynomiaux ont une précision locale et donc nécessite l'utilisation d'un maillage ce qui n'est pas le cas des modèles type réseaux de neurones qui ont une précision globale. 

- APPROXIMATION
T : Maintenant que nous avons défini l'encodeur et le décodeur, nous pouvons passer à l'approximateur.

14. L'idée de l'approximateur est le même que précédemment mais cette fois-ci on souhaite projeter une certaine forme de notre équation sur notre variété M_N. Ainsi, le problème continu écrit à gauche, où J est une fonctionnelle à minimiser sur notre variété, se ramène à un problème sur les degrés de liberté (donc un problème discret). On va alors distinguer deux variantes. La première concernera uniquement les EDP spatiales symétriques où notre fonctionnelle J prendra la forme énergétique de notre EDP, ce qui correspondra à la méthode appelée Deep-Ritz (et qui est comparable à la projection de Galerkin). La seconde variante est valable pour tout type d'EDP et consiste cette fois-ci à considérer notre fonctionnelle J comme la forme moindre-carré de notre EDP, ce qui correspondra à la méthode appelée PINNs standard (et qui est comparable à la projection de Gallerkin moindre-carré). 

15. Ainsi pour la méthode Deep-Ritz on va considérer le même problème de minimisation discret (incluant les conditions de bord) avec J_in notre forme énergétique. Et cette fois-ci, on discrétise les fonctions de coût par un processus aléatoire avec une méthode de Monte-Carlo. 

En fait, on va considérer 2 processus de génération aléatoire, un à l'intérieur de Omega et l'autre sur le bord. On notera également que les poids devant les fonctions de coûts reste à déterminer.

16. De la même manière, pour les méthode PINNs standard, on va considérer le problème de minimisation discret (incluant les conditions de bord) avec J_in notre forme moindre-carré et de la même manière on discrétise les fonctions de coût par un processus aléatoire avec une méthode de Mont

17. En pratique, pour résoudre ces deux problèmes de minimisation discrets définis précédemment pour la méthode Deep-Ritz et PINNs standard, on utilise une méthode de descente de gradients stochastiques par mini-batch, comme la méthode ADAM. On utilise également un modèle régulier dérivable plusieurs fois et le principe de différenciation automatique, ainsi que des fonctions d'activation suffisamment régulières pour être dérivées deux fois (à cause du laplacien). Par exemple une tangente hyperbolique à la place d'un relu, ou des méthodes adaptatives où on paramétrise les fonctions d'activation. On peut faire également deux remarques pour aller plus loin. La première est que dans le cadre des PINNs standard, on a la possibilité d'ajouter une fonction de coût appelée J_data qui permet d'approximer des solutions déjà connues. On, on peut également imposer les conditions de bord de manière exacte en utilisant une fonction levelset, ce que l'on va décrire dans la suite.

18. Maintenant en décomposant une méthode type apprentissage physiquement informé de la même manière que précédemment, on voit que la philosophie derrière ces deux types de méthodes est en fait très similaire. La principale différence est en fait dans l'espace d'approximation de la solution que l'on va considérer (dans le cadre de FEM des espaces vectoriels et dans le cadre des NN des variétés).

OUR HYBRID METHOD
T : Finissons par parler rapidement de la méthode hybride que nous sommes en train de développer.

19. Tout d'abord, laissez-moi vous parler rapidement des principales idées de la méthode PhiFM. La première idée est comme je vous le disais précédemment que le domaine est défini par ce qu'on appelle une fonction LevelSet, que l'on note phi. Cette fonction est définie comme étant nulle au bord, négative à l'intérieur de l'oméga et positive à l'extérieur.  La second idée et qu'on ne cherche pas à trouver la solution u mais on cherche w telle que la solution soit le forme suivante. On voit que le premier terme est nul au bord puis on y ajoute la condition de Dirichlet g. Et ainsi, dans le cadre de PhiFEM, le décodeur s'écrit de la manière suivante. Un dernier point important de la méthode PhiFEM est que on n'utilise pas un maillage de Omega mais un maillage d'un domaine fictif autour de Omega incluant Omega.

20. Avec cette même idée simple, on va pouvoir imposer les conditions au bord de manière exacte dans les PINNs. On considère alors notre solution comme étant de la même forme que dans le cadre de la méthode PhiFEM ou cette fois-ci w_theta est notre décodeur défini par un réseau de neurones comme un MLP. On considère alors le même problème de minimisation en enlevant cette fois-ci la fonction de coût associée au bord. 

21. Le pipeline est alors le suivant : On considère une géométrie définie par une fonction Levelset, une force f et possiblement une fonction décrivant nos conditions de bord g. On récupère la prédiction du PINNs où on impose les conditions au bord de manière exacte. Puis, on corrige la prédiction de notre réseau de neurones en utilisant la méthode PhiFEM. Une des méthodes les plus simples pour corriger la prédiction est ce que nous appelons la correction par addition. Elle consiste simplement à écrire la solution sous la forme du réseau de neurones auquel on ajouter ce qu'on va noter C_tild et ainsi on résout le problème suivant qui est équivalent au problème elliptic introduit au début de cette présentation. En fait on cherche simplement C telle que C_tild = phi*C, de sorte à ce que C_tild approche en fait la différence entre notre prédiction et la solution exacte de notre EDP. Si la prédiction est plutôt correcte, C_tild est assez proche de 0 et on espère que u_tild soit meilleure que la prédiction.
On peut faire rapidement une remarque importante. En fait, en imposant les conditions de manière exacte dans le PINNS, on obtient en général de meilleurs résultats que dans le cas où on doit minimiser la fonctionnelle J_bc. Cette méthode a donc un réel avantage mais peut avoir un coût si on ne possède pas la fonction phi. Comme on se place dans le cas où on a une fonction LevelSet, autant l'utiliser.

CONCLUSION
T : Passons à la conclusion !

22. Commençons par ce que nous avons vu. Tout d'abord, les méthodes d'apprentissage physiquement informées sont simplement une extension des méthodes numériques classiques comme FEM, où le décodeur est en fait une variété dont les propriétés sont différentes de ceux des espaces vectoriels. Par la suite, ces approches ont de réels avantages en grandes dimensions, particulièrement dans le cas des EDP paramétriques. De plus, comme ce sont des méthodes qui ne nécessitent pas de maillage, ils ont un réel avantage dans le contexte de géométrie complexe. 

23. Finalement, pour conclure sur notre approche hybride. L'intérêt de cette méthode est qu'elle combine La rapidité des réseaux de neurones à prédire une solution et la précision des méthodes FEM à corriger et certifier la prédiction de réseaux de neurones qui peuvent être complètement fausses sur un dataset inconnu par exemple. De plus, dans le contexte de géométrie complexe (ou dans les domaines applicatifs tels que le temps réel ou l'optimisation de forme), comme les réseaux de neurones, PhiFEM permet de ne pas avoir à (ré)-générer des maillages. Actuellement, nous avons eut certains résultats encourageants sur des géométries simples comme le cercle. Cependant, nous avons plus de difficultés sur des géométries complexes due à l'importance de la régularité de nos fonctions levelset. La prochaine étape est l'apprentissage de ces fonctions levelset.