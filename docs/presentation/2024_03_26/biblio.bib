@article{sukumar_exact_2022,
	title = {Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks},
	volume = {389},
	issn = {00457825},
	url = {http://arxiv.org/abs/2104.08426},
	doi = {10.1016/j.cma.2021.114333},
	abstract = {In this paper, we introduce a new approach based on distance fields to exactly impose boundary conditions in physics-informed deep neural networks. The challenges in satisfying Dirichlet boundary conditions in meshfree and particle methods are well-known. This issue is also pertinent in the development of physics informed neural networks (PINN) for the solution of partial differential equations. We introduce geometry-aware trial functions in artifical neural networks to improve the training in deep learning for partial differential equations. To this end, we use concepts from constructive solid geometry (R-functions) and generalized barycentric coordinates (mean value potential fields) to construct $\phi$, an approximate distance function to the boundary of a domain. To exactly impose homogeneous Dirichlet boundary conditions, the trial function is taken as $\phi$ multiplied by the PINN approximation, and its generalization via transfinite interpolation is used to a priori satisfy inhomogeneous Dirichlet (essential), Neumann (natural), and Robin boundary conditions on complex geometries. In doing so, we eliminate modeling error associated with the satisfaction of boundary conditions in a collocation method and ensure that kinematic admissibility is met pointwise in a Ritz method. We present numerical solutions for linear and nonlinear boundary-value problems over domains with affine and curved boundaries. Benchmark problems in 1D for linear elasticity, advection-diffusion, and beam bending; and in 2D for the Poisson equation, biharmonic equation, and the nonlinear Eikonal equation are considered. The approach extends to higher dimensions, and we showcase its use by solving a Poisson problem with homogeneous Dirichlet boundary conditions over the 4D hypercube. This study provides a pathway for meshfree analysis to be conducted on the exact geometry without domain discretization.},
	urldate = {2023-10-04},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Sukumar, N. and Srivastava, Ankit},
	month = feb,
	year = {2022},
	note = {arXiv:2104.08426 [cs, math]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Mathematics - Numerical Analysis},
	pages = {114333},
	file = {arXiv Fulltext PDF:/home/flecourtier/Zotero/storage/W4DS3562/Sukumar et Srivastava - 2022 - Exact imposition of boundary conditions with dista.pdf:application/pdf;arXiv.org Snapshot:/home/flecourtier/Zotero/storage/ERA5RC6D/2104.html:text/html},
}

@article{clemot_neural_2023,
	title = {Neural skeleton: {Implicit} neural representation away from the surface},
	volume = {114},
	shorttitle = {Neural skeleton},
	url = {https://hal.science/hal-04159959},
	doi = {10.1016/j.cag.2023.06.012},
	abstract = {Implicit Neural Representations are powerful tools for representing 3D shapes. They encode an implicit field in the parameters of a Neural Network, leveraging the power of auto-differentiation for optimizing the implicit function and avoiding the need for a manually crafted function. So far, Implicit Neural Representations have been mainly designed to extract or render object surfaces and methods primarily focus on improving the implicit function near the surface. In this paper we argue that implicit fields are useful for other shape analysis tasks, in particular skeleton (medial axis) extraction. Indeed, a medial axis is defined through distances to the surface, which can be provided by an implicit neural representation, making it robust to noise and missing data. However this requires the implicit field to be reliable away from the surface, something most representations are not optimized for. To achieve this, inspired by variational image denoising techniques, we propose to add a Total Variation term, to regularize the implicit field. We further design a skeleton sampling method working directly on the GPU, and link the extracted points using a coverage formulation. We show that our resulting neural skeleton is more robust to sample defects such as noise or missing data compared to other medial axis extraction methods.},
	urldate = {2024-03-21},
	journal = {Computers and Graphics},
	author = {Clémot, Mattéo and Digne, Julie},
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {368--378},
	file = {HAL PDF Full Text:/home/flecourtier/Zotero/storage/MQSB6953/Clémot et Digne - 2023 - Neural skeleton Implicit neural representation aw.pdf:application/pdf},
}

@article{fayolle_signed_2021,
	title = {Signed {Distance} {Function} {Computation} from an {Implicit} {Surface}},
	url = {http://arxiv.org/abs/2104.08057},
	doi = {10.48550/arXiv.2104.08057},
	abstract = {We describe in this short note a technique to convert an implicit surface into a Signed Distance Function (SDF) while exactly preserving the zero level-set of the implicit. The proposed approach relies on embedding the input implicit in the final layer of a neural network, which is trained to minimize a loss function characterizing the SDF.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Fayolle, Pierre-Alain},
	month = jun,
	year = {2021},
	note = {arXiv:2104.08057 [cs, math]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/flecourtier/Zotero/storage/PRVVRUVB/Fayolle - 2021 - Signed Distance Function Computation from an Impli.pdf:application/pdf;arXiv.org Snapshot:/home/flecourtier/Zotero/storage/CAFA73YC/2104.html:text/html},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	urldate = {2024-03-21},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
	file = {ScienceDirect Snapshot:/home/flecourtier/Zotero/storage/SXSTRR94/S0021999118307125.html:text/html},
}

@article{wang_experts_2023,
	title = {An {Expert}'s {Guide} to {Training} {Physics}-informed {Neural} {Networks}},
	url = {http://arxiv.org/abs/2308.08468},
	doi = {10.48550/arXiv.2308.08468},
	abstract = {Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparison purposes. To this end, we also release a highly optimized library in JAX that can be used to reproduce all results reported in this paper, enable future research studies, as well as facilitate easy adaptation to new use-case scenarios.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Wang, Sifan and Sankaran, Shyam and Wang, Hanwen and Perdikaris, Paris},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08468 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/flecourtier/Zotero/storage/DZLUI6X3/Wang et al. - 2023 - An Expert's Guide to Training Physics-informed Neu.pdf:application/pdf;arXiv.org Snapshot:/home/flecourtier/Zotero/storage/9JALIJ44/2308.html:text/html},
}